{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/43/84/23ed6a1796480a6f1a2d38f2802901d078266bda38388954d01d3f2e821d/pip-20.1.1-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 66.3MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 9.0.1\n",
      "    Uninstalling pip-9.0.1:\n",
      "      Successfully uninstalled pip-9.0.1\n",
      "Successfully installed pip-20.1.1\n",
      "\u001b[33mYou are using pip version 20.1.1, however version 20.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: gym[atari] in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (0.17.1)\n",
      "Requirement already satisfied: scipy in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from gym[atari]) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from gym[atari]) (1.16.2)\n",
      "Requirement already satisfied: six in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from gym[atari]) (1.11.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from gym[atari]) (1.3.0)\n",
      "Collecting atari_py~=0.2.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/58/45/c2f6523aed89db6672b241fa1aafcfa54126c564be769c1360d298f03852/atari_py-0.2.6-cp36-cp36m-manylinux1_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 23.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from gym[atari]) (5.0.0)\n",
      "Requirement already satisfied: opencv-python in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from gym[atari]) (3.4.1.15)\n",
      "Requirement already satisfied: future in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\n",
      "Installing collected packages: atari-py\n",
      "Successfully installed atari-py-0.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install 'gym[atari]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## config \n",
    "end_game_reward = -100\n",
    "hidden_layers = [12,12]\n",
    "gamma = 0.99\n",
    "learning_rate = 0.0001\n",
    "internal = 100\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "w,h,d = env.observation_space.shape\n",
    "state_num = w * h * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    def __init__(self, state_size, num_of_actions, hidden_layers, learning_rate):\n",
    "        self.states = tf.placeholder(shape=(None, state_size), dtype=tf.float32, name='input_states')\n",
    "        self.acc_r = tf.placeholder(shape=None, dtype=tf.float32, name='accumalated_rewards')\n",
    "        self.actions = tf.placeholder(shape=None, dtype=tf.int32, name='actions')\n",
    "        layer = self.states\n",
    "        for i in range(len(hidden_layers)):\n",
    "            layer = tf.layers.dense(inputs=layer, units=hidden_layers[i], activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    name='hidden_layer_{}'.format(i+1))\n",
    "        self.last_layer = tf.layers.dense(inputs=layer, units=num_of_actions, activation=tf.nn.tanh,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name='output')\n",
    "        self.action_prob = tf.nn.softmax(self.last_layer)\n",
    "        self.log_policy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.last_layer, labels=self.actions)\n",
    "        self.cost = tf.reduce_mean(self.acc_r * self.log_policy)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-79e413fcde06>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "pg = PolicyGradient(state_size=state_num, num_of_actions=env.action_space.n,\n",
    "                    hidden_layers=hidden_layers, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "data = pd.DataFrame(columns=['game','steps','cost'])\n",
    "\n",
    "for g in range(1500):\n",
    "    game = g+1\n",
    "    done = False\n",
    "    ## init env\n",
    "    observation = env.reset()\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    steps = 0\n",
    "    print_stuff('Starting game {}'.format(game))\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        observation = observation.flatten()[np.newaxis, :]\n",
    "        \n",
    "        probs = sess.run(pg.action_prob, feed_dict={pg.states: observation}).flatten()\n",
    "        # choose the action \n",
    "        action = np.random.choice(env.action_space.n, p=probs)\n",
    "        ## According to the action, give the next state,reward and whether game over\n",
    "        next_state, r, done, _ = env.step(action)\n",
    "        if done and steps < env._max_episode_steps: r = end_game_reward\n",
    "        \n",
    "        # Save to memory:\n",
    "        states.append(observation)\n",
    "        rewards.append(r)\n",
    "        actions.append(action)\n",
    "        observation = next_state\n",
    "    print_stuff('Game {g} has ended after {s} steps.'.format(g=game, s=steps))\n",
    "    \n",
    "    discounted_acc_rewards = np.zeros_like(rewards)\n",
    "    s = 0.0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        s = s * gamma + rewards[i]\n",
    "        discounted_acc_rewards[i] = s\n",
    "    discounted_acc_rewards = zscore(discounted_acc_rewards)\n",
    "    \n",
    "    states, discounted_acc_rewards, actions = shuffle(states, discounted_acc_rewards, actions)\n",
    "#     print(np.array(states).shape)\n",
    "    c, _ = sess.run([pg.cost, pg.optimizer], feed_dict={pg.states: np.squeeze(states), \n",
    "                                                        pg.acc_r: discounted_acc_rewards,\n",
    "                                                        pg.actions: actions})    \n",
    "    \n",
    "    print_stuff('Cost: {}\\n----------'.format(c))\n",
    "    data = data.append({'game':game, 'steps':steps, 'cost':c}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "env.reset()\n",
    "for _ in range(5000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done: break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.13.1",
   "language": "python",
   "name": "tensorflow-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
